{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/e/emiliano.penaloza/rlhf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "import peft\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from utils import get_local_dir, get_local_run_dir, disable_dropout, init_distributed, get_open_port\n",
    "import os\n",
    "import hydra\n",
    "import torch.multiprocessing as mp\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import trainers\n",
    "import wandb\n",
    "import json\n",
    "import socket\n",
    "from typing import Optional, Set\n",
    "import resource\n",
    "from hnet.lora import LoRA_controller, LoRA\n",
    "from transformers import LlamaModel, LlamaConfig,LlamaForCausalLM\n",
    "from hnet.hypernet import HyperNetController, HyperNet, HyperNetLinear,PolicyWrapper\n",
    "from accelerate import Accelerator\n",
    "from hnet.lora import LoRA_controller, LoRA\n",
    "\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['HF_HOME'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['TORCH_HOME'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "cache_dir = '/home/mila/e/emiliano.penaloza/scratch/models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    'vicgalle/gpt2-open-instruct-v1', cache_dir=cache_dir, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=None).to('cuda')\n",
    "base_policy = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    'vicgalle/gpt2-open-instruct-v1', cache_dir=cache_dir, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=None).to('cuda')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    'vicgalle/gpt2-open-instruct-v1', cache_dir=cache_dir, use_fast=True)\n",
    "policy = policy.to('cuda')\n",
    "hypernet = HyperNet(16, 0, 64,16,10,64,10, 10,10,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig, OmegaConf\n",
    "configs_d = {'model': {'target_modules':['c_attn']},\n",
    "             'hnet':{'r':16}}\n",
    "config = OmegaConf.create(configs_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[  4.3320,   4.2891,   1.4424,  ...,   3.6055,   4.3867,   4.7578],\n",
       "         [  2.4395,   4.3594,  -0.3579,  ...,  -7.6445,  -3.6582,  -2.9473],\n",
       "         [-14.0156,  -9.8828, -12.2969,  ..., -16.9531, -14.9062, -15.5781],\n",
       "         ...,\n",
       "         [ -3.5781,  -3.4590,  -5.9727,  ...,  -3.4199,  -2.9629,  -3.0371],\n",
       "         [ -3.6172,  -3.5020,  -6.0117,  ...,  -3.4453,  -2.9980,  -3.0703],\n",
       "         [ -3.2852,  -3.1465,  -5.6836,  ...,  -3.2520,  -2.7402,  -2.8086]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<UnsafeViewBackward0>), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer(\"hello world\", return_tensors='pt', padding='max_length', max_length=1024, truncation=True)\n",
    "x = {k: v.to('cuda') for k, v in x.items()}\n",
    "policy(**x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key='transformer.h.0.attn.c_attn'\n",
      "key='transformer.h.1.attn.c_attn'\n",
      "key='transformer.h.2.attn.c_attn'\n",
      "key='transformer.h.3.attn.c_attn'\n",
      "key='transformer.h.4.attn.c_attn'\n",
      "key='transformer.h.5.attn.c_attn'\n",
      "key='transformer.h.6.attn.c_attn'\n",
      "key='transformer.h.7.attn.c_attn'\n",
      "key='transformer.h.8.attn.c_attn'\n",
      "key='transformer.h.9.attn.c_attn'\n",
      "key='transformer.h.10.attn.c_attn'\n",
      "key='transformer.h.11.attn.c_attn'\n"
     ]
    }
   ],
   "source": [
    "controller = LoRA_controller(config)\n",
    "controller.augmentLLM(policy)\n",
    "controller.freezeParams(policy, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[  4.3320,   4.2891,   1.4424,  ...,   3.6055,   4.3867,   4.7578],\n",
       "         [  2.4395,   4.3594,  -0.3579,  ...,  -7.6445,  -3.6582,  -2.9473],\n",
       "         [-14.0156,  -9.8828, -12.2969,  ..., -16.9531, -14.9062, -15.5781],\n",
       "         ...,\n",
       "         [ -3.5781,  -3.4590,  -5.9727,  ...,  -3.4199,  -2.9629,  -3.0371],\n",
       "         [ -3.6172,  -3.5020,  -6.0117,  ...,  -3.4453,  -2.9980,  -3.0703],\n",
       "         [ -3.2852,  -3.1465,  -5.6836,  ...,  -3.2520,  -2.7402,  -2.8086]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<UnsafeViewBackward0>), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "base_policy(**x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[  4.3320,   4.2891,   1.4424,  ...,   3.6055,   4.3867,   4.7578],\n",
       "         [  2.4395,   4.3594,  -0.3579,  ...,  -7.6445,  -3.6582,  -2.9473],\n",
       "         [-14.0156,  -9.8828, -12.2969,  ..., -16.9531, -14.9062, -15.5781],\n",
       "         ...,\n",
       "         [ -3.5781,  -3.4590,  -5.9727,  ...,  -3.4199,  -2.9629,  -3.0371],\n",
       "         [ -3.6172,  -3.5020,  -6.0117,  ...,  -3.4453,  -2.9980,  -3.0703],\n",
       "         [ -3.2852,  -3.1465,  -5.6836,  ...,  -3.2520,  -2.7402,  -2.8086]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<UnsafeViewBackward0>), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controller.change_forward('adaptor')\n",
    "policy(**x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernet = HyperNet(16, 0, 64,16,10,64,10, 10,10,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.change_forward('base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['HF_HOME'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['TORCH_HOME'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "cache_dir = '/home/mila/e/emiliano.penaloza/scratch/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "b= torch.zeros(10,10,requires_grad=True)\n",
    "a =torch.ones(10,10,requires_grad=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a@b).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 10., 10., 10., 10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10., 10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10., 10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10., 10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10., 10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10., 10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10., 10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10., 10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10., 10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10., 10., 10., 10., 10., 10.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl \n",
    "with open('/home/mila/e/emiliano.penaloza/direct-preference-optimization/notebooks/data/train_data.pkl','rb') as f:\n",
    "    train_data = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl \n",
    "with open('/home/mila/e/emiliano.penaloza/direct-preference-optimization/notebooks/data/test_data.pkl','rb') as f:\n",
    "    test_data = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 989)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/mila/e/emiliano.penaloza/direct-preference-optimization/notebooks/data/test_data.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     test_json \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_data)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/cvmfs/ai.mila.quebec/apps/arch/distro/python/3.10/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cvmfs/ai.mila.quebec/apps/arch/distro/python/3.10/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/cvmfs/ai.mila.quebec/apps/arch/distro/python/3.10/lib/python3.10/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 989)"
     ]
    }
   ],
   "source": [
    "import json \n",
    "with open('/home/mila/e/emiliano.penaloza/direct-preference-optimization/notebooks/data/test_data.json','r') as f:\n",
    "    test_json = json.load(f)\n",
    "print(f\"{len(test_data)=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data)=19959\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"{len(train_data)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d = model.transformer.h[0].attn.c_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50260, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv1d\n",
    "#give me a sample forward \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tensor = torch.randn(10, 15, 768)\n",
    "\n",
    "output = conv1d(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 15, 2304])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/e/emiliano.penaloza/rlhf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#load a transformer encoder \n",
    "from transformers import BertModel, BertTokenizer\n",
    "encoder = nn.TransformerEncoderLayer(d_model=768, nhead=12)\n",
    "encoder_full = nn.TransformerEncoder(encoder, num_layers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting 4 processes for FSDP training\n",
      "setting RLIMIT_NOFILE soft limit to 1048576 from 1048576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/cvmfs/ai.mila.quebec/apps/arch/distro/python/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/cvmfs/ai.mila.quebec/apps/arch/distro/python/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'foo' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/cvmfs/ai.mila.quebec/apps/arch/distro/python/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/cvmfs/ai.mila.quebec/apps/arch/distro/python/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'foo' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/cvmfs/ai.mila.quebec/apps/arch/distro/python/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/cvmfs/ai.mila.quebec/apps/arch/distro/python/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'foo' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/cvmfs/ai.mila.quebec/apps/arch/distro/python/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/cvmfs/ai.mila.quebec/apps/arch/distro/python/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'foo' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 2 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetting RLIMIT_NOFILE soft limit to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhard\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msoft\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m mp\u001b[38;5;241m.\u001b[39mset_start_method(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforkserver\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfoo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rlhf/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:239\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    235\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    236\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    237\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m start_method)\n\u001b[1;32m    238\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspawn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rlhf/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/rlhf/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:149\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with signal \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    142\u001b[0m             (error_index, name),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m             signal_name\u001b[38;5;241m=\u001b[39mname\n\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with exit code \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    151\u001b[0m             (error_index, exitcode),\n\u001b[1;32m    152\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    153\u001b[0m             error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    154\u001b[0m             exit_code\u001b[38;5;241m=\u001b[39mexitcode\n\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    157\u001b[0m original_trace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_queues[error_index]\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    158\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 2 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "import torch.multiprocessing as mp\n",
    "import torch\n",
    "import resource\n",
    "\n",
    "def foo(x):\n",
    "    print(f\"{x=}\")\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    world_size = torch.cuda.device_count()\n",
    "    print('starting', world_size, 'processes for FSDP training')\n",
    "    soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "    resource.setrlimit(resource.RLIMIT_NOFILE, (hard, hard))\n",
    "    print(f'setting RLIMIT_NOFILE soft limit to {hard} from {soft}')\n",
    "    mp.set_start_method('forkserver')\n",
    "    mp.spawn(foo, nprocs=world_size, args=(world_size), join=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/e/emiliano.penaloza/rlhf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import MultiheadAttention\n",
    "from transformers import LlamaModel, LlamaConfig,LlamaForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "import torch.nn as nn \n",
    "from argparse import ArgumentParser\n",
    "from functools import partial\n",
    "#if the directory is not the root of the project, change it\n",
    "os.chdir('/home/mila/e/emiliano.penaloza/RLPHF')\n",
    "import LoRA.loralib  as lora\n",
    "from notebooks.loss import *\n",
    "import pandas as pd \n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['HF_HOME'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['TORCH_HOME'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "cache_dir = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuration=LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = 'cpu'\n",
    "# Initializing a LLaMA llama-7b style configuration\n",
    "configuration = LlamaConfig()\n",
    "print(f\"{configuration=}\")\n",
    "\n",
    "configuration.num_hidden_layers = 2\n",
    "configuration.num_attention_heads = 16\n",
    "configuration.num_key_value_heads = 2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/tulu-2-dpo-13b\",cache_dir = cache_dir)\n",
    "model = LlamaForCausalLM(configuration).to(device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token_id is None:\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an argument parser \n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--loss\", type=str, help=\"path to the model\")\n",
    "parser.add_argument(\"--max_length\", type=int, help=\"path to the model\")\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json data /home/mila/e/emiliano.penaloza/RLPHF/notebooks/data/train_data.json\n",
    "import json\n",
    "\n",
    "# Initialize an empty list to store the JSON objects\n",
    "data = []\n",
    "\n",
    "# Open the file and read the lines\n",
    "with open('/home/mila/e/emiliano.penaloza/RLPHF/notebooks/data/train_data.json', 'r') as file:\n",
    "    for line in file:\n",
    "        # Parse each line as a JSON object and append it to the list\n",
    "        if line:\n",
    "            try:\n",
    "                # Parse each line as a JSON object and append it to the list\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line: {line}\\nError: {e}\")\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.if_chosen.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no match\n",
      "no match\n",
      "no match\n",
      "no match\n",
      "no match\n",
      "no match\n",
      "no match\n",
      "no match\n",
      "no match\n",
      "no match\n",
      "no match\n",
      "no match\n",
      "no match\n",
      "no match\n",
      "no match\n",
      "no match\n"
     ]
    }
   ],
   "source": [
    "#import display from ipython\n",
    "from IPython.display import display\n",
    "dfs_uid = df.groupby('user_id')\n",
    "processed_data = []\n",
    "for i,(uid, df_uid) in enumerate(dfs_uid):\n",
    "    df_utt = df_uid.groupby('interaction_id')\n",
    "    for j,(utt, df_utt) in enumerate(df_utt):\n",
    "\n",
    "        if len(df_utt) >=  2:\n",
    "            try:\n",
    "                if len(df_utt[df_utt['if_chosen'] == True]) >= 2:\n",
    "                    #take the one with the higher score \n",
    "                    df_score_1 = df_utt.score.values[0]\n",
    "                    df_score_2 = df_utt.score.values[1]\n",
    "                    if df_score_1 > df_score_2:\n",
    "                        chosen = df_utt['if_chosen'].values[0]\n",
    "                        non_chosen = df_utt['if_chosen'].values[1]\n",
    "                    elif df_score_1 < df_score_2:\n",
    "                        chosen = df_utt['if_chosen'].values[1]\n",
    "                        non_chosen = df_utt['if_chosen'].values[0]\n",
    "\n",
    "                    else:\n",
    "                 \n",
    "                        continue\n",
    "                else:\n",
    "                    chosen = df_utt[df_utt['if_chosen'] == True].model_response.values[0]\n",
    "                    non_chosen = df_utt[df_utt['if_chosen'] == False].model_response.values[0]\n",
    "                # print(f\"{df_utt[df_utt['if_chosen'] == False].model_response=}\")\n",
    "\n",
    "\n",
    "                user_prompt = df_utt['user_prompt'].values[0]\n",
    "                user_id = df_utt['user_id'].values[0]\n",
    "                processed_data.append([user_id, user_prompt, chosen, non_chosen])\n",
    "            except:\n",
    "                print(f\"{df_utt[df_utt['if_chosen'] == False].model_response=}\")\n",
    "                display(df_utt)\n",
    "                print('error')\n",
    "                raise Exception\n",
    "        else:\n",
    "            print('no match')\n",
    "        continue\n",
    "    # if i == 10:\n",
    "    #         print(f\"{processed_data=}\")\n",
    "    #         print(f\"{len(processed_data)=}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# device = 'cpu'\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, processed_data):\n",
    "        self.data = processed_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out = {}\n",
    "        out['uid'] = self.data[idx][0]\n",
    "        out['prompt'] = self.data[idx][1]\n",
    "        out['chosen'] = self.data[idx][2]\n",
    "        out['rejected'] = self.data[idx][3]\n",
    "        return out\n",
    "\n",
    "# Create a CustomDataset instance\n",
    "dataset = CustomDataset(processed_data)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True,collate_fn=partial(collate_fn, tokenizer=tokenizer, max_length=args.max_length, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.keys()=dict_keys(['uids', 'prompt_prefered_ids', 'prompt_disprefered_ids', 'prompt_prefered_mask', 'prompt_disprefered_mask', 'labels_prefered', 'labels_disprefered'])\n"
     ]
    }
   ],
   "source": [
    "for b in dataloader:\n",
    "    print(f\"{b.keys()=}\")\n",
    "\n",
    "    #tokenize non_chosen and preffered \n",
    "    model_output = model(input_ids=b['prompt_prefered_ids'], attention_mask=b['prompt_prefered_mask'])\n",
    "    \n",
    "    break\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no match\n",
      "no match\n",
      "no match\n",
      "no match\n",
      "no match\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the JSON objects\n",
    "#/home/mila/e/emiliano.penaloza/RLPHF/notebooks/data/{split}_data.csv does not exits\n",
    "split = 'test'\n",
    "import json\n",
    "import pandas as pd \n",
    "data = []\n",
    "\n",
    "# Open the file and read the lines\n",
    "\n",
    "with open(f'/home/mila/e/emiliano.penaloza/RLPHF/notebooks/data/{split}_data.json', 'r') as file:\n",
    "    for line in file:\n",
    "        # Parse each line as a JSON object and append it to the list\n",
    "        if line:\n",
    "            try:\n",
    "                # Parse each line as a JSON object and append it to the list\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line: {line}\\nError: {e}\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "dfs_uid = df.groupby('user_id')\n",
    "processed_data = []\n",
    "for i,(uid, df_uid) in enumerate(dfs_uid):\n",
    "    df_utt = df_uid.groupby('interaction_id')\n",
    "    for j,(utt, df_utt) in enumerate(df_utt):\n",
    "\n",
    "        if len(df_utt) >=  2:\n",
    "            try:\n",
    "                if len(df_utt[df_utt['if_chosen'] == True]) >= 2:\n",
    "                    #take the one with the higher score \n",
    "                    df_score_1 = df_utt.score.values[0]\n",
    "                    df_score_2 = df_utt.score.values[1]\n",
    "                    if df_score_1 > df_score_2:\n",
    "                        chosen = df_utt.model_response.values[0]\n",
    "                        non_chosen = df_utt.model_response.values[1]\n",
    "                    elif df_score_1 < df_score_2:\n",
    "                        chosen = df_utt.model_response.values[1]\n",
    "                        non_chosen = df_utt.model_response.values[0]\n",
    "\n",
    "                    else:\n",
    "                \n",
    "                        continue\n",
    "                else:\n",
    "                    chosen = df_utt[df_utt['if_chosen'] == True].model_response.values[0]\n",
    "                    non_chosen = df_utt[df_utt['if_chosen'] == False].model_response.values[0]\n",
    "                # print(f\"{df_utt[df_utt['if_chosen'] == False].model_response=}\")\n",
    "\n",
    "\n",
    "                user_prompt = df_utt['user_prompt'].values[0]\n",
    "                user_id = df_utt['user_id'].values[0]\n",
    "                processed_data.append([user_id, user_prompt, chosen, non_chosen])\n",
    "            except:\n",
    "                print(f\"{df_utt[df_utt['if_chosen'] == False].model_response=}\")\n",
    "\n",
    "                print('error')\n",
    "                raise Exception\n",
    "        else:\n",
    "            print('no match')\n",
    "        continue\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example[1]='who else do you think represents the most threat to them in the nfc?'\n",
      "['The biggest threats to the Cowboys in the NFC are likely the Eagles, 49ers, and Buccaneers. The Eagles have a dynamic offense and are the defending NFC East champs. The 49ers have a formidable defense and made it to the NFC Championship last season. And the Buccaneers still have Tom Brady leading a talented roster. Getting through any of those teams in the playoffs would be a serious challenge for Dallas.', 'The biggest threats to the Cowboys in the NFC are likely the Eagles, 49ers, and Buccaneers. The Eagles have a dynamic offense and are the defending NFC East champs. The 49ers have a formidable defense and made it to the NFC Championship last season. And the Buccaneers still have Tom Brady leading a talented roster. Getting through any of those teams in the playoffs would be a serious challenge for Dallas.']\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m responses \u001b[38;5;241m=\u001b[39m [example[\u001b[38;5;241m2\u001b[39m], example[\u001b[38;5;241m3\u001b[39m]]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(responses)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for example in processed_data:\n",
    "\n",
    "        prompt = '\\n\\n Human: ' + example[1] + \"\\n\\n Assistant:\"\n",
    "        if example[1] == \"who else do you think represents the most threat to them in the nfc?\":\n",
    "            print(f\"{example[1]=}\")\n",
    "            responses = [example[2], example[3]]\n",
    "            print(responses)\n",
    "            raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>turn</th>\n",
       "      <th>within_turn_id</th>\n",
       "      <th>conversation_type</th>\n",
       "      <th>user_prompt</th>\n",
       "      <th>model_response</th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_provider</th>\n",
       "      <th>score</th>\n",
       "      <th>if_chosen</th>\n",
       "      <th>included_in_balanced_subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [utterance_id, interaction_id, conversation_id, user_id, turn, within_turn_id, conversation_type, user_prompt, model_response, model_name, model_provider, score, if_chosen, included_in_balanced_subset]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.model_response == True ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>interaction_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>turn</th>\n",
       "      <th>within_turn_id</th>\n",
       "      <th>conversation_type</th>\n",
       "      <th>user_prompt</th>\n",
       "      <th>model_response</th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_provider</th>\n",
       "      <th>score</th>\n",
       "      <th>if_chosen</th>\n",
       "      <th>included_in_balanced_subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7935</th>\n",
       "      <td>ut23731</td>\n",
       "      <td>int6608</td>\n",
       "      <td>c2743</td>\n",
       "      <td>user521</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>unguided</td>\n",
       "      <td>who else do you think represents the most thre...</td>\n",
       "      <td>The biggest threats to the Cowboys in the NFC ...</td>\n",
       "      <td>claude-2.1</td>\n",
       "      <td>anthropic</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7936</th>\n",
       "      <td>ut23732</td>\n",
       "      <td>int6608</td>\n",
       "      <td>c2743</td>\n",
       "      <td>user521</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>unguided</td>\n",
       "      <td>who else do you think represents the most thre...</td>\n",
       "      <td>The biggest threats to the Cowboys in the NFC ...</td>\n",
       "      <td>claude-2.1</td>\n",
       "      <td>anthropic</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     utterance_id interaction_id conversation_id  user_id  turn  \\\n",
       "7935      ut23731        int6608           c2743  user521     1   \n",
       "7936      ut23732        int6608           c2743  user521     1   \n",
       "\n",
       "      within_turn_id conversation_type  \\\n",
       "7935               0          unguided   \n",
       "7936               1          unguided   \n",
       "\n",
       "                                            user_prompt  \\\n",
       "7935  who else do you think represents the most thre...   \n",
       "7936  who else do you think represents the most thre...   \n",
       "\n",
       "                                         model_response  model_name  \\\n",
       "7935  The biggest threats to the Cowboys in the NFC ...  claude-2.1   \n",
       "7936  The biggest threats to the Cowboys in the NFC ...  claude-2.1   \n",
       "\n",
       "     model_provider  score  if_chosen  included_in_balanced_subset  \n",
       "7935      anthropic      9       True                         True  \n",
       "7936      anthropic      6       True                         True  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.user_prompt ==  \"who else do you think represents the most threat to them in the nfc?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     1, 10604, 29901,\n",
       "           450,  2315,   297,   278,  5057, 15194,   338,  3625,   304,  4840,\n",
       "           373, 16631, 15512, 13987,   297,  8314, 29889,   887,   508,  1804,\n",
       "           701,   363,   263,  3889, 14260,   470, 25691,   304, 16631, 15512,\n",
       "           304,  6505,   278,  1510, 29889,   739, 29915, 29879,   884,  3625,\n",
       "           304, 20590,   470, 23990,   373,  5087,  7412,   470,   474, 29911,\n",
       "          7844, 29889],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "             1, 10604, 29901,  8221,   304,  8293,   393, 29889,   739, 29915,\n",
       "         29879,  4100,   304,  7536,   277,   675,   596,  1914,  1532, 29899,\n",
       "           915,   292,   322,  2050,   278,  7037, 27721,   310,   596,  8820,\n",
       "         29889,   922,  1416,   292,  2304,   515,  9311,   287,  7875, 29892,\n",
       "          3942, 29892,   470,   263, 10257,  2613,  2838,   272,   508,  1371,\n",
       "           366, 23624,   445,  5189,  6434, 29889,   739, 29915, 29879,   884,\n",
       "          4100,   304,  2050,   278, 21737,   322,  1532, 29899,   915,   292,\n",
       "           310,   278,  8300,   767,  9701, 29892,   408,  1532,   408,   738,\n",
       "          7037, 10311,   393,  1033,  2041,   304,   670, 13718,   470, 21702,\n",
       "           411,  4045, 29889, 18514, 15084, 29892,   278, 10608,   373,   825,\n",
       "           304,   437,  2446,   338, 15850, 29892,   541,   372, 29915, 29879,\n",
       "          4100,   304,  2125,   278,   931,   304, 16112,  2050,   596,  3987,\n",
       "           322,  7536,   277,   675,   596,  1914, 23023,  1848,   322, 19119,\n",
       "          9045, 29889]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['labels_prefered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 408, 32000])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 640])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['prompt_prefered_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['prompt_prefered_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mb\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_prefered_mask\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "b['prompt_prefered_mask'][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
