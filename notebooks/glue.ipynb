{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/e/emiliano.penaloza/rlhf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#if the directory is not the root of the project, change it\n",
    "os.chdir('/home/mila/e/emiliano.penaloza/RLPHF')\n",
    "import torch.nn as nn \n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "# %%\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.nn import MultiheadAttention\n",
    "from transformers import LlamaModel, LlamaConfig,LlamaForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from notebooks.hypernet import HyperNetController, HyperNet, HyperNetLinear\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['HF_HOME'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['TORCH_HOME'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "cache_dir = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "\n",
    "\n",
    "# %%\n",
    "import copy \n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx', 'text', 'dataset', 'sentence'],\n",
      "    num_rows: 7336\n",
      "})\n",
      "\n",
      "Test dataset:\n",
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx', 'text', 'dataset', 'sentence'],\n",
      "    num_rows: 1834\n",
      "})\n",
      "\n",
      "Task distribution in train set:\n",
      "dataset\n",
      "sst2    0.600055\n",
      "mrpc    0.399945\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Task distribution in test set:\n",
      "dataset\n",
      "sst2    0.599782\n",
      "mrpc    0.400218\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: bert-base-uncased\n",
      "Model is on device: cuda\n",
      "initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memilianopp\u001b[0m (\u001b[33memilianouw\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mila/e/emiliano.penaloza/RLPHF/wandb/run-20240626_163955-07hj9pzk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/emilianouw/your_project_name/runs/07hj9pzk/workspace' target=\"_blank\">your_run_name</a></strong> to <a href='https://wandb.ai/emilianouw/your_project_name' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/emilianouw/your_project_name' target=\"_blank\">https://wandb.ai/emilianouw/your_project_name</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/emilianouw/your_project_name/runs/07hj9pzk/workspace' target=\"_blank\">https://wandb.ai/emilianouw/your_project_name/runs/07hj9pzk/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = 1\n",
    "alpha = 0.1\n",
    "dropout = 0.1\n",
    "n_transformer_layers = 1\n",
    "n_transformer_heads = 2\n",
    "d_emb = 64\n",
    "d_model = 16\n",
    "a_b = 2 \n",
    "kvq =3\n",
    "#this is to produce a layer at a time\n",
    "output_dim = (r *  d_model ) * a_b  * kvq   \n",
    "a = 16 \n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "sst2 = load_dataset(\"nyu-mll/glue\", \"sst2\")\n",
    "mrpc = load_dataset(\"nyu-mll/glue\", \"mrpc\")\n",
    "\n",
    "# Function to process MRPC dataset\n",
    "def process_mrpc(example):\n",
    "    return {\n",
    "        \"text\": example[\"sentence1\"] + \" \" + example[\"sentence2\"],\n",
    "        \"label\": example[\"label\"],\n",
    "        \"dataset\": \"mrpc\"\n",
    "    }\n",
    "\n",
    "# Process MRPC dataset (only the 'train' split)\n",
    "mrpc_processed = mrpc['train'].map(process_mrpc)\n",
    "\n",
    "# Function to process SST2 dataset\n",
    "def process_sst2(example):\n",
    "    return {\n",
    "        \"text\": example[\"sentence\"],\n",
    "        \"label\": example[\"label\"],\n",
    "        \"dataset\": \"sst2\"\n",
    "    }\n",
    "\n",
    "# Process SST2 dataset (only the 'train' split)\n",
    "sst2_processed = sst2['train'].map(process_sst2)\n",
    "\n",
    "# Convert both datasets to pandas DataFrames\n",
    "df_mrpc = mrpc_processed.to_pandas()\n",
    "df_sst2 = sst2_processed.to_pandas()\n",
    "\n",
    "# Calculate the number of samples to keep for SST2\n",
    "n_mrpc = len(df_mrpc)\n",
    "n_sst2_to_keep = int(n_mrpc * 1.5)  # 60% SST2, 40% MRPC\n",
    "\n",
    "# Downsample SST2\n",
    "df_sst2_downsampled = df_sst2.sample(n=n_sst2_to_keep, random_state=42)\n",
    "\n",
    "# Combine the DataFrames\n",
    "df_combined = pd.concat([df_mrpc, df_sst2_downsampled], ignore_index=True)\n",
    "\n",
    "# Ensure all columns have the same dtype\n",
    "df_combined[\"text\"] = df_combined[\"text\"].astype(str)\n",
    "df_combined[\"label\"] = df_combined[\"label\"].astype(int)\n",
    "df_combined[\"dataset\"] = df_combined[\"dataset\"].astype(str)\n",
    "\n",
    "# Create a stratified train-test split\n",
    "train_df, test_df = train_test_split(df_combined, test_size=0.2, stratify=df_combined['dataset'], random_state=42)\n",
    "\n",
    "# Shuffle the datasets\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Convert back to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Print info about the final datasets\n",
    "print(\"Train dataset:\")\n",
    "print(train_dataset)\n",
    "print(\"\\nTest dataset:\")\n",
    "print(test_dataset)\n",
    "\n",
    "# Verify the distribution of tasks in each split\n",
    "print(\"\\nTask distribution in train set:\")\n",
    "print(train_df['dataset'].value_counts(normalize=True))\n",
    "print(\"\\nTask distribution in test set:\")\n",
    "print(test_df['dataset'].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "#subset the train_dataset to only have text label and dataset \n",
    "train_dataset = train_dataset.remove_columns(['idx'])\n",
    "test_dataset = test_dataset.remove_columns(['idx'])\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(['sentence1','sentence2','sentence'])\n",
    "test_dataset = test_dataset.remove_columns(['sentence1','sentence2','sentence'])\n",
    "\n",
    "# %%\n",
    "#laod train_df into a dataloader \n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# %%\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model is on device: {device}\")\n",
    "\n",
    "# %%\n",
    "class_encodings = nn.Parameter(torch.empty(2,64))\n",
    "\n",
    "nn.init.xavier_normal_(class_encodings)\n",
    "print('initialized')\n",
    "\n",
    "# %%\n",
    "\n",
    "r = 8\n",
    "alpha = 0.1\n",
    "dropout = 0.1\n",
    "n_transformer_layers = 1\n",
    "n_transformer_heads = 2\n",
    "d_emb = 64\n",
    "d_model = 16\n",
    "a_b = 2 \n",
    "kvq =3\n",
    "#this is to produce a layer at a time\n",
    "output_dim = (r *  d_model ) * a_b  * kvq   \n",
    "a = 16 \n",
    "\n",
    "#load bert as the model \n",
    "# model = BertModel.from_pretrained('bert-base-uncased',cache_dir  = cache_dir).to(0)\n",
    "A = model.config.hidden_size\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"your_project_name\", name=\"your_run_name\")\n",
    "\n",
    "# Define hyperparameters\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\": 32,\n",
    "    \"r\": r,\n",
    "    \"alpha\": alpha,\n",
    "    \"dropout\": dropout,\n",
    "    \"output_dim\": output_dim,\n",
    "    \"d_emb\": d_emb,\n",
    "    \"d_model\": d_model,\n",
    "    \"n_transformer_layers\": n_transformer_layers,\n",
    "    \"n_transformer_heads\": n_transformer_heads,\n",
    "    \"a\": a,\n",
    "    \"A\": A\n",
    "}\n",
    "\n",
    "wandb.config.update(hyperparameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, hypernet, controller, eval_loader, task_mapper, device):\n",
    "    model.eval()\n",
    "    hypernet.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Initialize dictionaries to keep track of predictions per task\n",
    "    task_correct_predictions = {}\n",
    "    task_total_predictions = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b in eval_loader:\n",
    "            encodings = tokenizer(b['text'], truncation=True, padding=True, return_tensors='pt')\n",
    "            labels = b['label'].to(device)\n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "            tasks = [task_mapper[task] for task in b['dataset']]\n",
    "\n",
    "            task_embedding = torch.stack([class_encodings[task] for task in tasks]).unsqueeze(1).repeat(1,12,1).to(device)\n",
    "            identity_matrix = torch.eye(d_model).unsqueeze(0).repeat(input_ids.shape[0], 1, 1).to(device)[:,:12,:]\n",
    "            hypernet_layers = hypernet(task_embedding, identity_matrix).view(input_ids.shape[0], n_layers*kvq, r, -1)\n",
    "            controller.updateLayers(hypernet_layers)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            # Update task-specific predictions\n",
    "            for task, pred, label in zip(tasks, predictions, labels):\n",
    "                if task not in task_correct_predictions:\n",
    "                    task_correct_predictions[task] = 0\n",
    "                    task_total_predictions[task] = 0\n",
    "                task_correct_predictions[task] += (pred == label).item()\n",
    "                task_total_predictions[task] += 1\n",
    "\n",
    "    avg_loss = total_loss / len(eval_loader)\n",
    "\n",
    "    # Calculate accuracy per task\n",
    "    task_accuracy = {task: task_correct_predictions[task] / task_total_predictions[task] for task in task_correct_predictions}\n",
    "\n",
    "    return avg_loss, task_accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model, hypernet, and controller\n",
    "hypernet = HyperNet(r, alpha, dropout, output_dim, d_emb, d_model, n_transformer_layers, n_transformer_heads, a=a).to(0)\n",
    "controller = HyperNetController(hypernet, target_modules=['query', 'key', 'value'], r=r, A=A, a=a)\n",
    "controller.augmentLLM(model)\n",
    "controller.freezeParams(model, False)\n",
    "\n",
    "# Set up optimizers\n",
    "# model_trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
    "#hypernet and calss encodings\n",
    "hypernetParameters = [param for param in hypernet.parameters() if param.requires_grad] + [class_encodings]\n",
    "# modelOptimizer = torch.optim.Adam(model_trainable_params, lr=hyperparameters['learning_rate'])\n",
    "hypernetOptimizer = torch.optim.AdamW(hypernetParameters, lr=hyperparameters['learning_rate'])\n",
    "\n",
    "# Evaluation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5581818181818182, 0.4418181818181818)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = test_df[test_df['dataset'] == 'sst2'].label\n",
    "prop_1 = sum(labels)/len(labels)\n",
    "prop_0 = 1 - prop_1\n",
    "prop_1, prop_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6771117166212534, 0.32288828337874664)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = test_df[test_df['dataset'] == 'mrpc'].label\n",
    "prop_1 = sum(labels)/len(labels)\n",
    "prop_0 = 1 - prop_1\n",
    "prop_1, prop_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_embedding.shape=torch.Size([32, 12, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m task_embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([class_encodings[task] \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_embedding\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m\n\u001b[1;32m     25\u001b[0m identity_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(d_model,requires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)[:,:\u001b[38;5;241m12\u001b[39m,:]\n\u001b[1;32m     26\u001b[0m hypernet_layers \u001b[38;5;241m=\u001b[39m hypernet(task_embedding, identity_matrix)\u001b[38;5;241m.\u001b[39mview(input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], n_layers\u001b[38;5;241m*\u001b[39mkvq, r, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "task_mapper = {'mrpc': 0, 'sst2': 1}\n",
    "device = 'cuda'\n",
    "n_layers = 12\n",
    "for epoch in range(hyperparameters['epochs']):\n",
    "    model.train()\n",
    "    hypernet.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for k,b in tqdm(enumerate(train_loader), desc=f\"Epoch {epoch+1}/{hyperparameters['epochs']}\"):\n",
    "        # modelOptimizer.zero_grad()\n",
    "        hypernetOptimizer.zero_grad()\n",
    "\n",
    "        encodings = tokenizer(b['text'], truncation=True, padding=True, return_tensors='pt')\n",
    "        labels = b['label'].to(device)\n",
    "        input_ids = encodings['input_ids'].to(device)\n",
    "        attention_mask = encodings['attention_mask'].to(device)\n",
    "        tasks = [task_mapper[task] for task in b['dataset']]\n",
    "\n",
    "\n",
    "        task_embedding = torch.stack([class_encodings[task] for task in tasks]).unsqueeze(1).repeat(1,12,1).to(device)\n",
    "\n",
    "\n",
    "        identity_matrix = torch.eye(d_model,requires_grad = False).unsqueeze(0).repeat(input_ids.shape[0], 1, 1).to(device)[:,:12,:]\n",
    "        hypernet_layers = hypernet(task_embedding, identity_matrix).view(input_ids.shape[0], n_layers*kvq, r, -1)\n",
    "\n",
    "        controller.updateLayers(hypernet_layers)\n",
    "\n",
    "        loss = model(input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "      \n",
    "        # modelOptimizer.step()\n",
    "        hypernetOptimizer.step()\n",
    "        # if k == 0:\n",
    "        \n",
    "    eval_loss, eval_accuracy = evaluate(model, hypernet, controller, test_loader, task_mapper, device)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # Evaluation\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"eval_loss\": eval_loss\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{hyperparameters['epochs']}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Eval Loss: {eval_loss:.4f}\")\n",
    "    print(f\"Eval Accuracy: {eval_accuracy}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters for the hypernet: 96992\n"
     ]
    }
   ],
   "source": [
    "#get number of  trainable parameters for hypernet \n",
    "\n",
    "total_params = sum(p.numel() for p in hypernet.parameters())\n",
    "print(\"Total number of parameters for the hypernet:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',cache_dir  = cache_dir).to(0)\n",
    "#add peft lora model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.embeddings.position_embeddings.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.embeddings.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.embeddings.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.self.key.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.self.value.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.self.query.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.self.query.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.self.key.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.self.key.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.self.value.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.self.value.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.self.query.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.self.query.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.self.key.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.self.key.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.self.value.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.self.value.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.self.query.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.self.query.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.self.key.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.self.key.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.self.value.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.self.value.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.self.query.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.self.query.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.self.key.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.self.key.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.self.value.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.self.value.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.self.query.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.self.query.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.self.key.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.self.key.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.self.value.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.self.value.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.self.query.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.self.query.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.self.key.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.self.key.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.self.value.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.self.value.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.self.query.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.self.query.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.self.key.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.self.key.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.self.value.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.self.value.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.self.query.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.self.query.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.self.key.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.self.key.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.self.value.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.self.value.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.self.query.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.self.query.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.self.key.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.self.key.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.self.value.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.self.value.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.self.query.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.self.query.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.self.key.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.self.key.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.self.value.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.self.value.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.self.query.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.self.query.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.self.key.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.self.key.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.self.value.hyperAdapterA\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.self.value.hyperAdapterB\n",
      "True\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "False\n",
      "\n",
      "\n",
      "bert.pooler.dense.weight\n",
      "False\n",
      "\n",
      "\n",
      "bert.pooler.dense.bias\n",
      "False\n",
      "\n",
      "\n",
      "classifier.weight\n",
      "False\n",
      "\n",
      "\n",
      "classifier.bias\n",
      "False\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n,m in model.named_parameters():\n",
    "    print(n)\n",
    "    print(m.requires_grad)\n",
    "    print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
