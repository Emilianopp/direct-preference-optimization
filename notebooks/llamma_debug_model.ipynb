{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/e/emiliano.penaloza/rlhf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import MultiheadAttention\n",
    "from transformers import LlamaModel, LlamaConfig,LlamaForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import torch.nn as nn \n",
    "#if the directory is not the root of the project, change it\n",
    "os.chdir('/home/mila/e/emiliano.penaloza/RLPHF')\n",
    "import LoRA.loralib  as lora\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['HF_HOME'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['TORCH_HOME'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "cache_dir = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuration=LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.39.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initializing a LLaMA llama-7b style configuration\n",
    "configuration = LlamaConfig()\n",
    "print(f\"{configuration=}\")\n",
    "\n",
    "configuration.num_hidden_layers = 2\n",
    "configuration.num_attention_heads = 16\n",
    "configuration.num_key_value_heads = 2\n",
    "model = LlamaForCausalLM(configuration).to(0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 6738415616\n",
      "Number of trainable parameters: 6.74B\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_trainable_params = count_parameters(model)\n",
    "print(f'Number of trainable parameters: {num_trainable_params}')\n",
    "def human_format(num):\n",
    "    magnitude = 0\n",
    "    while abs(num) >= 1000:\n",
    "        magnitude += 1\n",
    "        num /= 1000.0\n",
    "    return '%.2f%s' % (num, ['', 'K', 'M', 'B', 'T'][magnitude])\n",
    "\n",
    "num_trainable_params = count_parameters(model)\n",
    "print(f'Number of trainable parameters: {human_format(num_trainable_params)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "\n",
    "class HyperNetController():\n",
    "    def __init__(self,hyper_net, r ,  A, a , target_modules = ['q_proj','k_proj','v_proj']):\n",
    "        self.hyper_net = hyper_net\n",
    "        self.target_modules = target_modules\n",
    "        self.hypernet_layers = []\n",
    "        self.d_emb = hyper_net.d_emb\n",
    "        self.r = r \n",
    "        self.A = A\n",
    "        self.a = a \n",
    "        # self.num_layers = num_layers\n",
    "        \n",
    "    def setHyperNetLayer(self,w0):\n",
    "        self.hypernet_layers.append( HyperNetLinear(self,w0,self.d_emb,self.d_emb,r = self.r,A = self.A, a = self.a))\n",
    "        return self.hypernet_layers[-1]\n",
    "    \n",
    "    def replace_module(self, parent_module, child_name, new_module, old_module):\n",
    "        setattr(parent_module, child_name, new_module)\n",
    "        new_module.weight = old_module.weight\n",
    "        if old_module.bias is not None:\n",
    "            new_module.bias = old_module.bias\n",
    "        if getattr(old_module, \"state\", None) is not None:\n",
    "            new_module.state = old_module.state\n",
    "            new_module.to(old_module.weight.device)\n",
    "\n",
    "        # dispatch to correct device\n",
    "        for name, module in new_module.named_modules():\n",
    "\n",
    "\n",
    "            if \"lora_\" in name:\n",
    "                \n",
    "                module.to(old_module.weight.device)    \n",
    "    def get_submodules(self, key):\n",
    "        parent = self.model.get_submodule(\".\".join(key.split(\".\")[:-1]))\n",
    "        target_name = key.split(\".\")[-1]\n",
    "        target = self.model.get_submodule(key)\n",
    "        return parent, target, target_name\n",
    "    \n",
    "    def augmentLLM(self, model):\n",
    "        \n",
    "        self.model = model\n",
    "        key_list = [key for key, _ in model.named_modules()]\n",
    "        for key in key_list:\n",
    "            if isinstance(self.target_modules,str):\n",
    "                target_module_found = re.fullmatch(self.target_modules, key)\n",
    "            else: \n",
    "                target_module_found = any(key.endswith(target_key) for target_key in self.target_modules)\n",
    "\n",
    "\n",
    "            if target_module_found:\n",
    "                parent,target,targe_name = self.get_submodules(key)\n",
    "                new_module = self.setHyperNetLayer( target.weight)\n",
    "                self.replace_module(parent, targe_name, new_module, target)\n",
    "                print('replaced',key)\n",
    "\n",
    "    def setLayerWeights(self,hypernet_outputs, layer):\n",
    "        assert hypernet_outputs.shape[1] == self.num_layers \n",
    "        for l,hyper_out in zip(self.hypernet_layers,hypernet_outputs):\n",
    "            for k,_ in enumerate(self.target_modules):\n",
    "                \n",
    "                l = self.hypernet_layers[l]\n",
    "    def updateLayers(self,new_layer_tensor):\n",
    "\n",
    "        #new_layer_tensor is of shape b X (l * kvq) X r X a*2 \n",
    "        for i,l in enumerate(self.hypernet_layers,):\n",
    "            #l_new is of shape b X r X a*2 \n",
    "            l_new = new_layer_tensor[:,i]\n",
    "\n",
    "            a = l_new[:,:,:self.a ]\n",
    "            b = l_new[:,:,self.a: ]\n",
    "            l.set_adapter(a ,b )\n",
    "            \n",
    "            \n",
    "                \n",
    "        \n",
    "\n",
    "class HyperNet(nn.Module):\n",
    "    def __init__(self, r, alpha, dropout, output_emb,d_emb, d_model,n_transformer_layers,hypernet_heads = 2,target_modules = ['q_proj','k_proj','v_proj'],a=16):\n",
    "        super(HyperNet, self).__init__()\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.dropout = dropout\n",
    "        self.n_transformer_layers = n_transformer_layers\n",
    "        self.d_emb = d_emb\n",
    "        self.a = a \n",
    "\n",
    "        \n",
    "        #each attention head is 3*(d_emb * d_emb) as each k,v,h matrix is d_emb * d_emb\n",
    "        #We want to use an r-ranked represtation of the d_emb * d_emb matrix so we decompose the total output parameters by r \n",
    "        #We replace d_emb by a singular r rankned vector\n",
    "\n",
    "        \n",
    "        # Initialize \n",
    "        # encoder layers with user_emb_dim\n",
    "        self.target_modules = target_modules\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=d_emb, nhead=hypernet_heads, dim_feedforward=d_emb * 4, dropout=dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=n_transformer_layers)\n",
    "        \n",
    "        #make an mlp to transform the output of the encoder into d_model \n",
    "        self.mlp_encoder = nn.Sequential(\n",
    "            nn.Linear(d_emb, d_emb * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_emb * 2, d_model)\n",
    "        )\n",
    "        # Initialize transformer decoder layers with d_emb\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(d_model=d_model, nhead=hypernet_heads, dim_feedforward=d_emb * 4, dropout=dropout)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=n_transformer_layers)\n",
    "        \n",
    "        # MLP to map the decoder output to self.net_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model* 2, self.a  * 2 *3  )\n",
    "        )\n",
    "        \n",
    "    def forward(self, task_embedding, identity_matrix):\n",
    "        \n",
    "        # Encode the task embedding\n",
    "        encoded_task = self.transformer_encoder(task_embedding)\n",
    "        encoded_task = self.mlp_encoder(encoded_task)\n",
    "        \n",
    "        # Decode to generate the network parameters\n",
    "        decoded_output = self.transformer_decoder(identity_matrix, encoded_task)\n",
    "        \n",
    "        # Map the decoder output to the network parameters using MLP\n",
    "        params = self.mlp(decoded_output)\n",
    "        # params = self.organize_outputs(params)\n",
    "        return params\n",
    "    \n",
    "    def organize_outputs(self, outputs):\n",
    "        param_l = []\n",
    "        for out in outputs:\n",
    "            sub_list = []\n",
    "            s = 0 \n",
    "            for target in self.target_modules:\n",
    "                \n",
    "                sub_list.append(out[s:2 *(s+self.r) ])\n",
    "                s = 2* (s + self.r)\n",
    "            param_l.append(sub_list)\n",
    "        return param_l\n",
    "            \n",
    "\n",
    "            \n",
    "class HyperNetLinear(nn.Linear):\n",
    "    def __init__(self,hypernet,w0,in_features,out_features,r = 1 , scaling = 1, A=512, a = 16):\n",
    "        nn.Linear.__init__(self,in_features,out_features)\n",
    "        self.scaling = scaling\n",
    "        self.hypernet = hypernet\n",
    "        self.w0 = copy.deepcopy(w0)\n",
    "\n",
    "        self.adapterA = nn.init.xavier_normal_(torch.empty(A, r)).to(w0.device)\n",
    "        self.adapterB = nn.init.xavier_normal_(torch.empty(A, r)).to(w0.device)\n",
    "\n",
    "        self.orthA = self.make_orth(A,a).to(w0.device)\n",
    "        self.orthB = self.make_orth(A,a).to(w0.device)\n",
    "    def make_orth(self,A,a):\n",
    "        gaus = torch.randn (A,A )\n",
    "        svd = torch.linalg.svd (gaus)        \n",
    "        return (svd[0] @ svd[2])[:a]\n",
    "\n",
    "    def set_adapter(self,adapterA,adapterB,transposeA = True):\n",
    "        self.adapterA = adapterA if not transposeA else torch.transpose(adapterA,1,2)\n",
    "        self.adapterB = adapterB\n",
    "        \n",
    "    def forward(self,x):\n",
    "        bsize = x.shape[0]\n",
    "        orthA = self.orthA.T.repeat(bsize,1,1)\n",
    "        orthB = self.orthB.repeat(bsize,1,1)\n",
    "        _A = torch.bmm(orthA , self.adapterA)\n",
    "        _B = torch.bmm( self.adapterB,orthB)\n",
    "        out = x @ (self.scaling * (self.w0 + _A @ _B  ))\n",
    "        raise Exception\n",
    "        return out \n",
    "    def train(self,mode = True):\n",
    "        #Make sure the adapters are in training mode but the orth matrices are not and w0 are not \n",
    "        self.adapterA.train(mode)\n",
    "        self.adapterB.train(mode)\n",
    "        self.orthA.train(False)\n",
    "        self.orthB.train(False)\n",
    "        self.w0.train(False)\n",
    "        return self\n",
    "        \n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "r = 1\n",
    "alpha = 0.1\n",
    "dropout = 0.1\n",
    "n_transformer_layers = 1\n",
    "n_transformer_heads = 2\n",
    "d_emb = 64\n",
    "d_model = 16\n",
    "a_b = 2 \n",
    "kvq =3\n",
    "#this is to produce a layer at a time\n",
    "output_dim = (r *  d_model ) * a_b  * kvq   \n",
    "a = 16 \n",
    "\n",
    "#load bert as the model \n",
    "model = BertModel.from_pretrained('bert-base-uncased',cache_dir  = cache_dir).to(0)\n",
    "A = model.config.hidden_size\n",
    "\n",
    "hypernet = HyperNet(r, alpha, dropout, output_dim,d_emb, d_model, n_transformer_layers, n_transformer_heads,a = a ).to(0)\n",
    "controller = HyperNetController(hypernet,target_modules = ['query','key','value'], \n",
    "                                r = r,\n",
    "                                A = A,\n",
    "                                a = a)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaced encoder.layer.0.attention.self.query\n",
      "replaced encoder.layer.0.attention.self.key\n",
      "replaced encoder.layer.0.attention.self.value\n",
      "replaced encoder.layer.1.attention.self.query\n",
      "replaced encoder.layer.1.attention.self.key\n",
      "replaced encoder.layer.1.attention.self.value\n",
      "replaced encoder.layer.2.attention.self.query\n",
      "replaced encoder.layer.2.attention.self.key\n",
      "replaced encoder.layer.2.attention.self.value\n",
      "replaced encoder.layer.3.attention.self.query\n",
      "replaced encoder.layer.3.attention.self.key\n",
      "replaced encoder.layer.3.attention.self.value\n",
      "replaced encoder.layer.4.attention.self.query\n",
      "replaced encoder.layer.4.attention.self.key\n",
      "replaced encoder.layer.4.attention.self.value\n",
      "replaced encoder.layer.5.attention.self.query\n",
      "replaced encoder.layer.5.attention.self.key\n",
      "replaced encoder.layer.5.attention.self.value\n",
      "replaced encoder.layer.6.attention.self.query\n",
      "replaced encoder.layer.6.attention.self.key\n",
      "replaced encoder.layer.6.attention.self.value\n",
      "replaced encoder.layer.7.attention.self.query\n",
      "replaced encoder.layer.7.attention.self.key\n",
      "replaced encoder.layer.7.attention.self.value\n",
      "replaced encoder.layer.8.attention.self.query\n",
      "replaced encoder.layer.8.attention.self.key\n",
      "replaced encoder.layer.8.attention.self.value\n",
      "replaced encoder.layer.9.attention.self.query\n",
      "replaced encoder.layer.9.attention.self.key\n",
      "replaced encoder.layer.9.attention.self.value\n",
      "replaced encoder.layer.10.attention.self.query\n",
      "replaced encoder.layer.10.attention.self.key\n",
      "replaced encoder.layer.10.attention.self.value\n",
      "replaced encoder.layer.11.attention.self.query\n",
      "replaced encoder.layer.11.attention.self.key\n",
      "replaced encoder.layer.11.attention.self.value\n"
     ]
    }
   ],
   "source": [
    "controller.augmentLLM(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
