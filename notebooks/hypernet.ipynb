{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import MultiheadAttention\n",
    "from transformers import LlamaModel, LlamaConfig,LlamaForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import torch.nn as nn \n",
    "#if the directory is not the root of the project, change it\n",
    "os.chdir('/home/mila/e/emiliano.penaloza/RLPHF')\n",
    "import LoRA.loralib  as lora\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['HF_HOME'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['TORCH_HOME'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "cache_dir = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[181], line 27\u001b[0m\n\u001b[1;32m     20\u001b[0m hypernet \u001b[38;5;241m=\u001b[39m HyperNet(r, alpha, dropout, output_dim,d_emb, d_model, n_transformer_layers, n_transformer_heads,a \u001b[38;5;241m=\u001b[39m a )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     21\u001b[0m controller \u001b[38;5;241m=\u001b[39m HyperNetController(hypernet,target_modules \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     22\u001b[0m                                 r \u001b[38;5;241m=\u001b[39m r,\n\u001b[1;32m     23\u001b[0m                                 A \u001b[38;5;241m=\u001b[39m A,\n\u001b[1;32m     24\u001b[0m                                 a \u001b[38;5;241m=\u001b[39m a)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mcontroller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugmentLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[144], line 59\u001b[0m, in \u001b[0;36mHyperNetController.augmentLLM\u001b[0;34m(self, model, verbose)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_module_found:\n\u001b[1;32m     58\u001b[0m     parent,target,targe_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_submodules(key)\n\u001b[0;32m---> 59\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetHyperNetLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplace_module(parent, targe_name, new_module, target)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "Cell \u001b[0;32mIn[144], line 21\u001b[0m, in \u001b[0;36mHyperNetController.setHyperNetLayer\u001b[0;34m(self, w0)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetHyperNetLayer\u001b[39m(\u001b[38;5;28mself\u001b[39m,w0):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypernet_layers\u001b[38;5;241m.\u001b[39mappend( \u001b[43mHyperNetLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypernet_layers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[144], line 173\u001b[0m, in \u001b[0;36mHyperNetLinear.__init__\u001b[0;34m(self, hypernet, w0, in_features, out_features, r, scaling, A, a)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Register orthA and orthB as buffers instead of Parameters\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morthA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_orth(A, a)\u001b[38;5;241m.\u001b[39mto(w0\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morthB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_orth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(w0\u001b[38;5;241m.\u001b[39mdevice))\n",
      "Cell \u001b[0;32mIn[144], line 177\u001b[0m, in \u001b[0;36mHyperNetLinear.make_orth\u001b[0;34m(self, A, a)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_orth\u001b[39m(\u001b[38;5;28mself\u001b[39m, A, a):\n\u001b[1;32m    176\u001b[0m     gaus \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(A, A)\n\u001b[0;32m--> 177\u001b[0m     svd \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgaus\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (svd[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m@\u001b[39m svd[\u001b[38;5;241m2\u001b[39m])[:a]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "r = 1\n",
    "alpha = 0.1\n",
    "dropout = 0.1\n",
    "n_transformer_layers = 1\n",
    "n_transformer_heads = 2\n",
    "d_emb = 64\n",
    "d_model = 16\n",
    "a_b = 2 \n",
    "kvq =3\n",
    "#this is to produce a layer at a time\n",
    "output_dim = (r *  d_model ) * a_b  * kvq   \n",
    "a = 16 \n",
    "\n",
    "#load bert as the model \n",
    "model = BertModel.from_pretrained('bert-base-uncased',cache_dir  = cache_dir).to(0)\n",
    "A = model.config.hidden_size\n",
    "\n",
    "hypernet = HyperNet(r, alpha, dropout, output_dim,d_emb, d_model, n_transformer_layers, n_transformer_heads,a = a ).to(0)\n",
    "controller = HyperNetController(hypernet,target_modules = ['query','key','value'], \n",
    "                                r = r,\n",
    "                                A = A,\n",
    "                                a = a)\n",
    "\n",
    "\n",
    "controller.augmentLLM(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_embedding.shape=torch.Size([10, 12, 64])\n",
      "layers.shape=torch.Size([10, 12, 16])\n",
      "new_layers.shape=torch.Size([10, 36, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "# Create a random task embedding with dimension user_emb_dim\n",
    "bsize  = 10 \n",
    "d_emb_user = 64\n",
    "n_layers = 12\n",
    "task_embedding = torch.randn((bsize,n_layers,(d_emb_user))).to(0)\n",
    "print(f\"{task_embedding.shape=}\")\n",
    "\n",
    "# Create an identity matrix with dimensions matching the batch size and d_emb\n",
    "layers = torch.eye(d_model ).unsqueeze(0).repeat(bsize, 1, 1)[:,:n_layers,:].to(0)\n",
    "print(f\"{layers.shape=}\")\n",
    "# new_layers = hypernet(task_embedding,layers).view(bsize,n_layers*kvq,r,-1)\n",
    "print(f\"{new_layers.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed something in laguage to the bert model b\n",
    "\n",
    "input_ids = torch.randint(0, 1000, (bsize, 20)).to(0)\n",
    "attention_mask = torch.ones_like(input_ids).to(0)\n",
    "# output = model(input_ids,attention_mask = attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 768]), torch.Size([10, 20, 768]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.pooler_output.shape,output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = load_dataset(\"nyu-mll/glue\", \"sst2\")\n",
    "mrpc = load_dataset(\"nyu-mll/glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx', 'text', 'dataset', 'sentence'],\n",
      "    num_rows: 7336\n",
      "})\n",
      "\n",
      "Test dataset:\n",
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx', 'text', 'dataset', 'sentence'],\n",
      "    num_rows: 1834\n",
      "})\n",
      "\n",
      "Task distribution in train set:\n",
      "dataset\n",
      "sst2    0.600055\n",
      "mrpc    0.399945\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Task distribution in test set:\n",
      "dataset\n",
      "sst2    0.599782\n",
      "mrpc    0.400218\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the datasets\n",
    "sst2 = load_dataset(\"nyu-mll/glue\", \"sst2\")\n",
    "mrpc = load_dataset(\"nyu-mll/glue\", \"mrpc\")\n",
    "\n",
    "# Function to process MRPC dataset\n",
    "def process_mrpc(example):\n",
    "    return {\n",
    "        \"text\": example[\"sentence1\"] + \" \" + example[\"sentence2\"],\n",
    "        \"label\": example[\"label\"],\n",
    "        \"dataset\": \"mrpc\"\n",
    "    }\n",
    "\n",
    "# Process MRPC dataset (only the 'train' split)\n",
    "mrpc_processed = mrpc['train'].map(process_mrpc)\n",
    "\n",
    "# Function to process SST2 dataset\n",
    "def process_sst2(example):\n",
    "    return {\n",
    "        \"text\": example[\"sentence\"],\n",
    "        \"label\": example[\"label\"],\n",
    "        \"dataset\": \"sst2\"\n",
    "    }\n",
    "\n",
    "# Process SST2 dataset (only the 'train' split)\n",
    "sst2_processed = sst2['train'].map(process_sst2)\n",
    "\n",
    "# Convert both datasets to pandas DataFrames\n",
    "df_mrpc = mrpc_processed.to_pandas()\n",
    "df_sst2 = sst2_processed.to_pandas()\n",
    "\n",
    "# Calculate the number of samples to keep for SST2\n",
    "n_mrpc = len(df_mrpc)\n",
    "n_sst2_to_keep = int(n_mrpc * 1.5)  # 60% SST2, 40% MRPC\n",
    "\n",
    "# Downsample SST2\n",
    "df_sst2_downsampled = df_sst2.sample(n=n_sst2_to_keep, random_state=42)\n",
    "\n",
    "# Combine the DataFrames\n",
    "df_combined = pd.concat([df_mrpc, df_sst2_downsampled], ignore_index=True)\n",
    "\n",
    "# Ensure all columns have the same dtype\n",
    "df_combined[\"text\"] = df_combined[\"text\"].astype(str)\n",
    "df_combined[\"label\"] = df_combined[\"label\"].astype(int)\n",
    "df_combined[\"dataset\"] = df_combined[\"dataset\"].astype(str)\n",
    "\n",
    "# Create a stratified train-test split\n",
    "train_df, test_df = train_test_split(df_combined, test_size=0.2, stratify=df_combined['dataset'], random_state=42)\n",
    "\n",
    "# Shuffle the datasets\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Convert back to Hugging Face datasets\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Print info about the final datasets\n",
    "print(\"Train dataset:\")\n",
    "print(train_dataset)\n",
    "print(\"\\nTest dataset:\")\n",
    "print(test_dataset)\n",
    "\n",
    "# Verify the distribution of tasks in each split\n",
    "print(\"\\nTask distribution in train set:\")\n",
    "print(train_df['dataset'].value_counts(normalize=True))\n",
    "print(\"\\nTask distribution in test set:\")\n",
    "print(test_df['dataset'].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "#subset the train_dataset to only have text label and dataset \n",
    "train_dataset = train_dataset.remove_columns(['idx'])\n",
    "test_dataset = test_dataset.remove_columns(['idx'])\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(['sentence1','sentence2','sentence'])\n",
    "test_dataset = test_dataset.remove_columns(['sentence1','sentence2','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#laod train_df into a dataloader \n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: bert-base-uncased\n",
      "Model is on device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Model is on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n"
     ]
    }
   ],
   "source": [
    "class_encodings = nn.Parameter(torch.empty(2,64))\n",
    "\n",
    "nn.init.xavier_normal_(class_encodings)\n",
    "print('initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "r = 1\n",
    "alpha = 0.1\n",
    "dropout = 0.1\n",
    "n_transformer_layers = 1\n",
    "n_transformer_heads = 2\n",
    "d_emb = 64\n",
    "d_model = 16\n",
    "a_b = 2 \n",
    "kvq =3\n",
    "#this is to produce a layer at a time\n",
    "output_dim = (r *  d_model ) * a_b  * kvq   \n",
    "a = 16 \n",
    "\n",
    "#load bert as the model \n",
    "# model = BertModel.from_pretrained('bert-base-uncased',cache_dir  = cache_dir).to(0)\n",
    "A = model.config.hidden_size\n",
    "\n",
    "hypernet = HyperNet(r, alpha, dropout, output_dim,d_emb, d_model, n_transformer_layers, n_transformer_heads,a = a ).to(0)\n",
    "controller = HyperNetController(hypernet,target_modules = ['query','key','value'], \n",
    "                                r = r,\n",
    "                                A = A,\n",
    "                                a = a)\n",
    "\n",
    "\n",
    "controller.augmentLLM(model)\n",
    "\n",
    "controller.freezeParams(model,False)\n",
    "\n",
    "model_trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
    "modelOptimizer = torch.optim.Adam(model_trainable_params, lr=1e-4)\n",
    "hypernetOptimizer = torch.optim.Adam(hypernet.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training mode\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "hypernet.train()    \n",
    "print('training mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_mapper = {'mrpc': 0 , 'sst2': 1}\n",
    "device = 'cuda'\n",
    "for b in train_loader:\n",
    "    modelOptimizer.zero_grad()\n",
    "    hypernetOptimizer.zero_grad()\n",
    "    encodings = tokenizer(b['text'], truncation=True, padding=True,return_tensors='pt')\n",
    "    labels = b['label'].to(device)\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "    tasks = [task_mapper[task] for task in b['dataset']]\n",
    "\n",
    "    task_embedding = torch.stack([class_encodings[task] for task in tasks]).unsqueeze(1).repeat(1,12,1).to(device)\n",
    "    identity_matrix = torch.eye(d_model).unsqueeze(0).repeat(input_ids.shape[0], 1, 1).to(device)[:,:12,:]\n",
    "    hypernet_layers = hypernet(task_embedding,identity_matrix).view(bsize,n_layers*kvq,r,-1)\n",
    "    controller.updateLayers(hypernet_layers)\n",
    "    \n",
    "    loss  = model(input_ids,attention_mask = attention_mask,labels = labels).loss\n",
    "    #optimizer stuff \n",
    "    loss.backward()\n",
    "    modelOptimizer.step()\n",
    "    hypernetOptimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7819, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.3513,  0.2113],\n",
       "        [-0.5490,  0.0690],\n",
       "        [-0.2109, -0.1376],\n",
       "        [-0.2421,  0.0646],\n",
       "        [-0.4282,  0.3292],\n",
       "        [-0.2840,  0.3000],\n",
       "        [-0.6149,  0.1521],\n",
       "        [-0.2951,  0.0040],\n",
       "        [-0.2611,  0.1063],\n",
       "        [-0.3548,  0.2396],\n",
       "        [-0.3537,  0.0051],\n",
       "        [-0.4265,  0.2425],\n",
       "        [-0.3525,  0.2373],\n",
       "        [-0.4346,  0.1305],\n",
       "        [-0.2350,  0.0970],\n",
       "        [-0.3173, -0.0028],\n",
       "        [-0.5806,  0.0109],\n",
       "        [-0.3009, -0.0507],\n",
       "        [-0.0360, -0.0360],\n",
       "        [-0.1475,  0.1545],\n",
       "        [-0.5010,  0.2160],\n",
       "        [-0.2557,  0.1231],\n",
       "        [-0.5865,  0.2402],\n",
       "        [-0.4791,  0.0761],\n",
       "        [-0.4312,  0.1473],\n",
       "        [-0.2364,  0.2963],\n",
       "        [-0.5053,  0.1519],\n",
       "        [-0.4846,  0.1573],\n",
       "        [-0.3797,  0.0935],\n",
       "        [-0.0753,  0.2044],\n",
       "        [ 0.0846,  0.1572],\n",
       "        [-0.2918,  0.2107]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids,attention_mask = attention_mask,labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer_encoder.layers.0.self_attn.in_proj_weight\n",
      "transformer_encoder.layers.0.self_attn.in_proj_bias\n",
      "transformer_encoder.layers.0.self_attn.out_proj.weight\n",
      "transformer_encoder.layers.0.self_attn.out_proj.bias\n",
      "transformer_encoder.layers.0.linear1.weight\n",
      "transformer_encoder.layers.0.linear1.bias\n",
      "transformer_encoder.layers.0.linear2.weight\n",
      "transformer_encoder.layers.0.linear2.bias\n",
      "transformer_encoder.layers.0.norm1.weight\n",
      "transformer_encoder.layers.0.norm1.bias\n",
      "transformer_encoder.layers.0.norm2.weight\n",
      "transformer_encoder.layers.0.norm2.bias\n",
      "mlp_encoder.0.weight\n",
      "mlp_encoder.0.bias\n",
      "mlp_encoder.2.weight\n",
      "mlp_encoder.2.bias\n",
      "transformer_decoder.layers.0.self_attn.in_proj_weight\n",
      "transformer_decoder.layers.0.self_attn.in_proj_bias\n",
      "transformer_decoder.layers.0.self_attn.out_proj.weight\n",
      "transformer_decoder.layers.0.self_attn.out_proj.bias\n",
      "transformer_decoder.layers.0.multihead_attn.in_proj_weight\n",
      "transformer_decoder.layers.0.multihead_attn.in_proj_bias\n",
      "transformer_decoder.layers.0.multihead_attn.out_proj.weight\n",
      "transformer_decoder.layers.0.multihead_attn.out_proj.bias\n",
      "transformer_decoder.layers.0.linear1.weight\n",
      "transformer_decoder.layers.0.linear1.bias\n",
      "transformer_decoder.layers.0.linear2.weight\n",
      "transformer_decoder.layers.0.linear2.bias\n",
      "transformer_decoder.layers.0.norm1.weight\n",
      "transformer_decoder.layers.0.norm1.bias\n",
      "transformer_decoder.layers.0.norm2.weight\n",
      "transformer_decoder.layers.0.norm2.bias\n",
      "transformer_decoder.layers.0.norm3.weight\n",
      "transformer_decoder.layers.0.norm3.bias\n",
      "mlp.0.weight\n",
      "mlp.0.bias\n",
      "mlp.2.weight\n",
      "mlp.2.bias\n"
     ]
    }
   ],
   "source": [
    "#print trainable parameters\n",
    "for name, param in hypernet.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
